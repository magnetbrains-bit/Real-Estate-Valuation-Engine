{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONlu1wMQr9M54WVu+J/LSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magnetbrains-bit/Real-Estate-Valuation-Engine/blob/main/Real_Estate_Valuation_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "FkyhsgdCQkGQ",
        "outputId": "7e969367-e4bd-46af-a15d-28a965d12307"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-df00ca7b-274d-4fb3-9129-d2a573b0f834\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-df00ca7b-274d-4fb3-9129-d2a573b0f834\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to upload your Kaggle API key\n",
        "from google.colab import files\n",
        "files.upload() # Choose the kaggle.json file you downloaded\n",
        "\n",
        "# Next, run this cell to move the key to the correct location\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset from Kaggle\n",
        "!kaggle datasets download -d ruchi798/housing-prices-in-metropolitan-areas-of-india\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip housing-prices-in-metropolitan-areas-of-india.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGc__GoHQx0J",
        "outputId": "cae75f5a-678c-49cc-b4de-77d6c009026c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ruchi798/housing-prices-in-metropolitan-areas-of-india\n",
            "License(s): CC0-1.0\n",
            "Downloading housing-prices-in-metropolitan-areas-of-india.zip to /content\n",
            "  0% 0.00/269k [00:00<?, ?B/s]\n",
            "100% 269k/269k [00:00<00:00, 590MB/s]\n",
            "Archive:  housing-prices-in-metropolitan-areas-of-india.zip\n",
            "  inflating: Bangalore.csv           \n",
            "  inflating: Chennai.csv             \n",
            "  inflating: Delhi.csv               \n",
            "  inflating: Hyderabad.csv           \n",
            "  inflating: Kolkata.csv             \n",
            "  inflating: Mumbai.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np # We'll need numpy for some cleaning steps\n",
        "\n",
        "# Load the Mumbai-specific dataset into a DataFrame\n",
        "df = pd.read_csv('Mumbai.csv')\n",
        "\n",
        "# Let's take a first look at our data\n",
        "print(\"Shape of the dataset:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nInfo about columns and null values:\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST5L7VWuQx3D",
        "outputId": "9abfa4ee-9ab5-4931-8a6d-0c1391e1ff01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the dataset: (7719, 40)\n",
            "\n",
            "First 5 rows:\n",
            "     Price  Area  Location  No. of Bedrooms  Resale  MaintenanceStaff  \\\n",
            "0  4850000   720  Kharghar                1       1                 1   \n",
            "1  4500000   600  Kharghar                1       1                 1   \n",
            "2  6700000   650  Kharghar                1       1                 1   \n",
            "3  4500000   650  Kharghar                1       1                 1   \n",
            "4  5000000   665  Kharghar                1       1                 1   \n",
            "\n",
            "   Gymnasium  SwimmingPool  LandscapedGardens  JoggingTrack  ...  \\\n",
            "0          0             0                  0             0  ...   \n",
            "1          1             1                  0             1  ...   \n",
            "2          1             1                  0             1  ...   \n",
            "3          0             0                  1             0  ...   \n",
            "4          0             0                  1             0  ...   \n",
            "\n",
            "   LiftAvailable  BED  VaastuCompliant  Microwave  GolfCourse  TV  \\\n",
            "0              1    0                1          0           0   0   \n",
            "1              1    0                1          0           0   0   \n",
            "2              1    0                1          0           0   0   \n",
            "3              1    1                1          0           0   0   \n",
            "4              1    0                1          0           0   0   \n",
            "\n",
            "   DiningTable  Sofa  Wardrobe  Refrigerator  \n",
            "0            0     0         0             0  \n",
            "1            0     0         0             0  \n",
            "2            0     0         0             0  \n",
            "3            0     0         1             0  \n",
            "4            0     0         0             0  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "\n",
            "Info about columns and null values:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7719 entries, 0 to 7718\n",
            "Data columns (total 40 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   Price                7719 non-null   int64 \n",
            " 1   Area                 7719 non-null   int64 \n",
            " 2   Location             7719 non-null   object\n",
            " 3   No. of Bedrooms      7719 non-null   int64 \n",
            " 4   Resale               7719 non-null   int64 \n",
            " 5   MaintenanceStaff     7719 non-null   int64 \n",
            " 6   Gymnasium            7719 non-null   int64 \n",
            " 7   SwimmingPool         7719 non-null   int64 \n",
            " 8   LandscapedGardens    7719 non-null   int64 \n",
            " 9   JoggingTrack         7719 non-null   int64 \n",
            " 10  RainWaterHarvesting  7719 non-null   int64 \n",
            " 11  IndoorGames          7719 non-null   int64 \n",
            " 12  ShoppingMall         7719 non-null   int64 \n",
            " 13  Intercom             7719 non-null   int64 \n",
            " 14  SportsFacility       7719 non-null   int64 \n",
            " 15  ATM                  7719 non-null   int64 \n",
            " 16  ClubHouse            7719 non-null   int64 \n",
            " 17  School               7719 non-null   int64 \n",
            " 18  24X7Security         7719 non-null   int64 \n",
            " 19  PowerBackup          7719 non-null   int64 \n",
            " 20  CarParking           7719 non-null   int64 \n",
            " 21  StaffQuarter         7719 non-null   int64 \n",
            " 22  Cafeteria            7719 non-null   int64 \n",
            " 23  MultipurposeRoom     7719 non-null   int64 \n",
            " 24  Hospital             7719 non-null   int64 \n",
            " 25  WashingMachine       7719 non-null   int64 \n",
            " 26  Gasconnection        7719 non-null   int64 \n",
            " 27  AC                   7719 non-null   int64 \n",
            " 28  Wifi                 7719 non-null   int64 \n",
            " 29  Children'splayarea   7719 non-null   int64 \n",
            " 30  LiftAvailable        7719 non-null   int64 \n",
            " 31  BED                  7719 non-null   int64 \n",
            " 32  VaastuCompliant      7719 non-null   int64 \n",
            " 33  Microwave            7719 non-null   int64 \n",
            " 34  GolfCourse           7719 non-null   int64 \n",
            " 35  TV                   7719 non-null   int64 \n",
            " 36  DiningTable          7719 non-null   int64 \n",
            " 37  Sofa                 7719 non-null   int64 \n",
            " 38  Wardrobe             7719 non-null   int64 \n",
            " 39  Refrigerator         7719 non-null   int64 \n",
            "dtypes: int64(39), object(1)\n",
            "memory usage: 2.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Handle missing values (if any) ---\n",
        "# Let's check for missing values again after our initial exploration\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "# If there were any, a simple approach is to drop them:\n",
        "df = df.dropna()\n",
        "print(\"\\nShape after handling null values:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkHmc7DcQx8Y",
        "outputId": "05d359d5-141c-448e-e124-26f24553b8e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            "Price                  0\n",
            "Area                   0\n",
            "Location               0\n",
            "No. of Bedrooms        0\n",
            "Resale                 0\n",
            "MaintenanceStaff       0\n",
            "Gymnasium              0\n",
            "SwimmingPool           0\n",
            "LandscapedGardens      0\n",
            "JoggingTrack           0\n",
            "RainWaterHarvesting    0\n",
            "IndoorGames            0\n",
            "ShoppingMall           0\n",
            "Intercom               0\n",
            "SportsFacility         0\n",
            "ATM                    0\n",
            "ClubHouse              0\n",
            "School                 0\n",
            "24X7Security           0\n",
            "PowerBackup            0\n",
            "CarParking             0\n",
            "StaffQuarter           0\n",
            "Cafeteria              0\n",
            "MultipurposeRoom       0\n",
            "Hospital               0\n",
            "WashingMachine         0\n",
            "Gasconnection          0\n",
            "AC                     0\n",
            "Wifi                   0\n",
            "Children'splayarea     0\n",
            "LiftAvailable          0\n",
            "BED                    0\n",
            "VaastuCompliant        0\n",
            "Microwave              0\n",
            "GolfCourse             0\n",
            "TV                     0\n",
            "DiningTable            0\n",
            "Sofa                   0\n",
            "Wardrobe               0\n",
            "Refrigerator           0\n",
            "dtype: int64\n",
            "\n",
            "Shape after handling null values: (7719, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Clean and standardize column names ---\n",
        "# Good practice to make them lowercase and replace spaces/dots with underscores\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('.', '')\n",
        "print(\"\\nDataFrame with cleaned column names:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXQOKTM4RtlY",
        "outputId": "4fbfc9f0-915b-4dcd-8e05-530981df1e67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame with cleaned column names:\n",
            "     price  area  location  no_of_bedrooms  resale  maintenancestaff  \\\n",
            "0  4850000   720  Kharghar               1       1                 1   \n",
            "1  4500000   600  Kharghar               1       1                 1   \n",
            "2  6700000   650  Kharghar               1       1                 1   \n",
            "3  4500000   650  Kharghar               1       1                 1   \n",
            "4  5000000   665  Kharghar               1       1                 1   \n",
            "\n",
            "   gymnasium  swimmingpool  landscapedgardens  joggingtrack  ...  \\\n",
            "0          0             0                  0             0  ...   \n",
            "1          1             1                  0             1  ...   \n",
            "2          1             1                  0             1  ...   \n",
            "3          0             0                  1             0  ...   \n",
            "4          0             0                  1             0  ...   \n",
            "\n",
            "   liftavailable  bed  vaastucompliant  microwave  golfcourse  tv  \\\n",
            "0              1    0                1          0           0   0   \n",
            "1              1    0                1          0           0   0   \n",
            "2              1    0                1          0           0   0   \n",
            "3              1    1                1          0           0   0   \n",
            "4              1    0                1          0           0   0   \n",
            "\n",
            "   diningtable  sofa  wardrobe  refrigerator  \n",
            "0            0     0         0             0  \n",
            "1            0     0         0             0  \n",
            "2            0     0         0             0  \n",
            "3            0     0         1             0  \n",
            "4            0     0         0             0  \n",
            "\n",
            "[5 rows x 40 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Ensure correct data types ---\n",
        "# The 'price' and 'area' columns look good (int64), but let's verify.\n",
        "# The 'no_of_bedrooms' column is perfect.\n",
        "# The amenity columns are already in a useful 0/1 format.\n",
        "# Let's check for any strange values in 'area' or 'price'.\n",
        "print(\"\\nLooking for outliers or strange values...\")\n",
        "print(df.describe())\n",
        "# A common issue is properties with unrealistic sizes or prices.\n",
        "# Let's filter out properties with an area less than 200 sqft, as they are likely data errors.\n",
        "df = df[df['area'] >= 200]\n",
        "print(f\"\\nShape after removing tiny properties: {df.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJQmteJPRtoY",
        "outputId": "6f75d89b-d798-477e-b54d-4359c8ba79dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Looking for outliers or strange values...\n",
            "              price         area  no_of_bedrooms       resale  \\\n",
            "count  7.719000e+03  7719.000000     7719.000000  7719.000000   \n",
            "mean   1.506165e+07   998.409250        1.913331     0.647105   \n",
            "std    2.052100e+07   550.967809        0.855376     0.477901   \n",
            "min    2.000000e+06   200.000000        1.000000     0.000000   \n",
            "25%    5.300000e+06   650.000000        1.000000     0.000000   \n",
            "50%    9.500000e+06   900.000000        2.000000     1.000000   \n",
            "75%    1.700000e+07  1177.000000        2.000000     1.000000   \n",
            "max    4.200000e+08  8511.000000        7.000000     1.000000   \n",
            "\n",
            "       maintenancestaff    gymnasium  swimmingpool  landscapedgardens  \\\n",
            "count       7719.000000  7719.000000   7719.000000        7719.000000   \n",
            "mean           7.498899     7.473896      7.437881           7.441638   \n",
            "std            3.197923     3.252095      3.328245           3.320401   \n",
            "min            0.000000     0.000000      0.000000           0.000000   \n",
            "25%            9.000000     9.000000      9.000000           9.000000   \n",
            "50%            9.000000     9.000000      9.000000           9.000000   \n",
            "75%            9.000000     9.000000      9.000000           9.000000   \n",
            "max            9.000000     9.000000      9.000000           9.000000   \n",
            "\n",
            "       joggingtrack  rainwaterharvesting  ...  liftavailable          bed  \\\n",
            "count   7719.000000          7719.000000  ...    7719.000000  7719.000000   \n",
            "mean       7.439435             7.477005  ...       7.518331     7.417930   \n",
            "std        3.325002             3.245418  ...       3.155041     3.369523   \n",
            "min        0.000000             0.000000  ...       0.000000     0.000000   \n",
            "25%        9.000000             9.000000  ...       9.000000     9.000000   \n",
            "50%        9.000000             9.000000  ...       9.000000     9.000000   \n",
            "75%        9.000000             9.000000  ...       9.000000     9.000000   \n",
            "max        9.000000             9.000000  ...       9.000000     9.000000   \n",
            "\n",
            "       vaastucompliant    microwave   golfcourse           tv  diningtable  \\\n",
            "count      7719.000000  7719.000000  7719.000000  7719.000000  7719.000000   \n",
            "mean          7.454722     7.372069     7.379712     7.379065     7.373624   \n",
            "std           3.292904     3.462108     3.446892     3.448185     3.459020   \n",
            "min           0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "25%           9.000000     9.000000     9.000000     9.000000     9.000000   \n",
            "50%           9.000000     9.000000     9.000000     9.000000     9.000000   \n",
            "75%           9.000000     9.000000     9.000000     9.000000     9.000000   \n",
            "max           9.000000     9.000000     9.000000     9.000000     9.000000   \n",
            "\n",
            "              sofa     wardrobe  refrigerator  \n",
            "count  7719.000000  7719.000000   7719.000000  \n",
            "mean      7.374530     7.383988      7.374789  \n",
            "std       3.457217     3.438345      3.456702  \n",
            "min       0.000000     0.000000      0.000000  \n",
            "25%       9.000000     9.000000      9.000000  \n",
            "50%       9.000000     9.000000      9.000000  \n",
            "75%       9.000000     9.000000      9.000000  \n",
            "max       9.000000     9.000000      9.000000  \n",
            "\n",
            "[8 rows x 39 columns]\n",
            "\n",
            "Shape after removing tiny properties: (7719, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Clean the 'location' column ---\n",
        "# Just like with the Bengaluru data, we'll group rare locations into an 'other' category.\n",
        "# This prevents the model from overfitting to locations with very few data points.\n",
        "df['location'] = df['location'].apply(lambda x: x.strip())\n",
        "location_stats = df['location'].value_counts()\n"
      ],
      "metadata": {
        "id": "UZWSqBDiRtrI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how many locations have fewer than 10 listings\n",
        "print(f\"\\nNumber of locations with <= 10 listings: {len(location_stats[location_stats <= 10])}\")\n",
        "\n",
        "# Let's set the threshold. Any location with 10 or fewer listings will be marked as 'other'.\n",
        "locations_less_than_10 = location_stats[location_stats <= 10]\n",
        "df['location'] = df['location'].apply(lambda x: 'other' if x in locations_less_than_10 else x)\n",
        "\n",
        "print(f\"\\nNumber of unique locations after cleaning: {df['location'].nunique()}\")\n",
        "print(\"\\nSample of location counts after grouping:\")\n",
        "print(df['location'].value_counts().head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o019KIUDRtt_",
        "outputId": "05aa2b91-004d-442c-aa1c-ec9125c5ee6f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of locations with <= 10 listings: 312\n",
            "\n",
            "Number of unique locations after cleaning: 102\n",
            "\n",
            "Sample of location counts after grouping:\n",
            "location\n",
            "other             824\n",
            "Kharghar          681\n",
            "Thane West        577\n",
            "Mira Road East    481\n",
            "Ulwe              391\n",
            "Nala Sopara       225\n",
            "Borivali West     202\n",
            "Kalyan West       197\n",
            "Andheri West      189\n",
            "Panvel            180\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's assume 'df' is your DataFrame after completing the initial cleaning steps.\n",
        "\n",
        "print(\"--- Starting Advanced Logical Cleaning ---\")\n",
        "print(f\"Shape before advanced cleaning: {df.shape}\")\n",
        "\n",
        "# --- 1. Feature Engineering: Create 'price_per_sqft' ---\n",
        "# The price is in Lakhs (100,000). We'll convert it to actual value for calculation.\n",
        "df['price_per_sqft'] = (df['price'] * 100000) / df['area']\n",
        "print(\"\\n'price_per_sqft' column created.\")\n",
        "print(df[['price', 'area', 'price_per_sqft']].head())\n",
        "print(\"\\nStatistics for price_per_sqft:\")\n",
        "print(df['price_per_sqft'].describe())\n",
        "\n",
        "\n",
        "# --- 2. Removing Extreme Price Outliers by Location ---\n",
        "# We will write a function to remove data points beyond 1 standard deviation from the mean\n",
        "# for each location. This is a standard data science technique.\n",
        "\n",
        "def remove_price_outliers(df_in):\n",
        "    df_out = pd.DataFrame()\n",
        "    for key, subdf in df_in.groupby('location'):\n",
        "        m = np.mean(subdf.price_per_sqft)\n",
        "        st = np.std(subdf.price_per_sqft)\n",
        "        reduced_df = subdf[(subdf.price_per_sqft > (m - st)) & (subdf.price_per_sqft <= (m + st))]\n",
        "        df_out = pd.concat([df_out, reduced_df], ignore_index=True)\n",
        "    return df_out\n",
        "\n",
        "df = remove_price_outliers(df)\n",
        "print(f\"\\nShape after removing price outliers: {df.shape}\")\n",
        "\n",
        "\n",
        "# --- 3. Removing Bedroom-vs-Area Outliers ---\n",
        "# It's a common data error to see, for example, a 6-bedroom house with 500 sqft.\n",
        "# We'll assume a typical bedroom needs at least 300 sqft on average.\n",
        "# So, we will remove properties where (area / no_of_bedrooms) is less than 300.\n",
        "\n",
        "df = df[~(df.area / df.no_of_bedrooms < 300)]\n",
        "print(f\"\\nShape after removing bedroom/area outliers: {df.shape}\")\n",
        "\n",
        "\n",
        "# --- Final Cleanup ---\n",
        "# Now that we've used price_per_sqft to clean, we can drop it before training the model.\n",
        "df = df.drop('price_per_sqft', axis='columns')\n",
        "print(\"\\nDropped 'price_per_sqft' column.\")\n",
        "\n",
        "print(\"\\n--- Advanced Cleaning Complete ---\")\n",
        "print(f\"Final shape of the cleaned dataset: {df.shape}\")\n",
        "# --- Overwrite the old cleaned CSV with our newly deep-cleaned data ---\n",
        "df.to_csv('cleaned_mumbai_data.csv', index=False)\n",
        "print(\"\\nSaved the deep-cleaned data to 'cleaned_mumbai_data.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxcHXP6mRtwW",
        "outputId": "c593b9ec-7677-4ca4-ee98-158cbb081673"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Advanced Logical Cleaning ---\n",
            "Shape before advanced cleaning: (6583, 40)\n",
            "\n",
            "'price_per_sqft' column created.\n",
            "      price  area  price_per_sqft\n",
            "0   6200000   400    1.550000e+09\n",
            "1   9500000  1000    9.500000e+08\n",
            "2  14900000  1245    1.196787e+09\n",
            "3  14000000  1183    1.183432e+09\n",
            "4   3600000  1245    2.891566e+08\n",
            "\n",
            "Statistics for price_per_sqft:\n",
            "count    6.583000e+03\n",
            "mean     1.211271e+09\n",
            "std      8.315487e+08\n",
            "min      8.078125e+07\n",
            "25%      6.215067e+08\n",
            "50%      9.595960e+08\n",
            "75%      1.597576e+09\n",
            "max      8.235294e+09\n",
            "Name: price_per_sqft, dtype: float64\n",
            "\n",
            "Shape after removing price outliers: (4782, 41)\n",
            "\n",
            "Shape after removing bedroom/area outliers: (4782, 41)\n",
            "\n",
            "Dropped 'price_per_sqft' column.\n",
            "\n",
            "--- Advanced Cleaning Complete ---\n",
            "Final shape of the cleaned dataset: (4782, 40)\n",
            "\n",
            "Saved the deep-cleaned data to 'cleaned_mumbai_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install and import necessary libraries ---\n",
        "!pip install geopy\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "import time # To add delays between requests\n",
        "\n",
        "# Load our clean dataset\n",
        "df = pd.read_csv('cleaned_mumbai_data.csv')\n",
        "\n",
        "# --- Step 2: Geocode the unique locations ---\n",
        "# IMPORTANT: We only geocode the UNIQUE locations, not the entire column.\n",
        "# This is much faster and avoids thousands of redundant API calls.\n",
        "\n",
        "# Initialize the geocoder\n",
        "geolocator = Nominatim(user_agent=\"mumbai_real_estate_app\")\n",
        "\n",
        "# Get the list of unique locations from our DataFrame\n",
        "unique_locations = df['location'].unique()\n",
        "\n",
        "# Create a dictionary to store the coordinates\n",
        "location_coords = {}\n",
        "\n",
        "print(f\"Geocoding {len(unique_locations)} unique locations. This may take a few minutes...\")\n",
        "\n",
        "for loc in unique_locations:\n",
        "    # We add \"Mumbai, India\" to make the search more specific and accurate\n",
        "    query = f\"{loc}, Mumbai, India\"\n",
        "    try:\n",
        "        location_data = geolocator.geocode(query)\n",
        "        if location_data:\n",
        "            location_coords[loc] = (location_data.latitude, location_data.longitude)\n",
        "        else:\n",
        "            location_coords[loc] = (None, None) # Location not found\n",
        "    except Exception as e:\n",
        "        print(f\"Error geocoding {loc}: {e}\")\n",
        "        location_coords[loc] = (None, None)\n",
        "\n",
        "    time.sleep(1) # IMPORTANT: Be polite to the free API by adding a 1-second delay\n",
        "\n",
        "print(\"\\nGeocoding complete!\")\n",
        "\n",
        "# --- Step 3: Map the coordinates back to our main DataFrame ---\n",
        "df['latitude'] = df['location'].map(lambda loc: location_coords.get(loc, (None, None))[0])\n",
        "df['longitude'] = df['location'].map(lambda loc: location_coords.get(loc, (None, None))[1])\n",
        "\n",
        "# Check how many locations we successfully geocoded\n",
        "print(f\"\\nNumber of properties with missing coordinates: {df['latitude'].isnull().sum()}\")\n",
        "\n",
        "# Drop any rows where we couldn't find coordinates\n",
        "df = df.dropna(subset=['latitude', 'longitude'])\n",
        "\n",
        "print(f\"Final shape after geocoding: {df.shape}\")\n",
        "print(\"\\nDataFrame with new latitude and longitude columns:\")\n",
        "print(df.head())\n",
        "\n",
        "# Save our newly enriched data!\n",
        "df.to_csv('geocoded_mumbai_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK-02oNtRtzR",
        "outputId": "75f7f098-31fa-4556-d5d4-f1687a3b8daf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
            "Geocoding 102 unique locations. This may take a few minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\")': /search?q=Bhayandar+East%2C+Mumbai%2C+India&format=json&limit=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Geocoding complete!\n",
            "\n",
            "Number of properties with missing coordinates: 1860\n",
            "Final shape after geocoding: (2922, 42)\n",
            "\n",
            "DataFrame with new latitude and longitude columns:\n",
            "      price  area location  no_of_bedrooms  resale  maintenancestaff  \\\n",
            "0   6200000   400   Airoli               1       1                 1   \n",
            "1   9500000  1000   Airoli               2       1                 1   \n",
            "2  14900000  1245   Airoli               2       0                 0   \n",
            "3  14000000  1183   Airoli               2       0                 0   \n",
            "4   6400000   495   Airoli               1       1                 9   \n",
            "\n",
            "   gymnasium  swimmingpool  landscapedgardens  joggingtrack  ...  \\\n",
            "0          0             0                  0             0  ...   \n",
            "1          0             1                  0             0  ...   \n",
            "2          0             0                  0             0  ...   \n",
            "3          1             1                  1             1  ...   \n",
            "4          9             9                  9             9  ...   \n",
            "\n",
            "   vaastucompliant  microwave  golfcourse  tv  diningtable  sofa  wardrobe  \\\n",
            "0                0          0           0   0            0     0         0   \n",
            "1                0          0           0   0            0     0         0   \n",
            "2                0          0           0   0            0     0         0   \n",
            "3                1          0           0   0            0     0         0   \n",
            "4                9          9           9   9            9     9         9   \n",
            "\n",
            "   refrigerator   latitude  longitude  \n",
            "0             0  19.158515  72.999402  \n",
            "1             0  19.158515  72.999402  \n",
            "2             0  19.158515  72.999402  \n",
            "3             0  19.158515  72.999402  \n",
            "4             9  19.158515  72.999402  \n",
            "\n",
            "[5 rows x 42 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install and import necessary libraries ---\n",
        "!pip install osmnx geopandas\n",
        "import osmnx as ox\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from scipy.spatial import cKDTree\n",
        "import numpy as np\n",
        "\n",
        "# Load our geocoded dataset\n",
        "df = pd.read_csv('geocoded_mumbai_data.csv')\n",
        "\n",
        "# --- Step 2: Download amenity data (e.g., Metro Stations) from OpenStreetMap ---\n",
        "print(\"Downloading metro station data from OpenStreetMap...\")\n",
        "tags = {\"railway\": \"station\", \"station\": \"subway\"}\n",
        "metro_stations = ox.features.features_from_place('Mumbai, India', tags)\n",
        "print(f\"Found {len(metro_stations)} metro stations.\")\n",
        "\n",
        "# --- DIAGNOSTIC STEP: See the different geometry types we have ---\n",
        "print(\"\\nTypes of geometries found for stations:\")\n",
        "print(metro_stations.geom_type.value_counts())\n",
        "\n",
        "\n",
        "# --- Step 3: Calculate the distance to the NEAREST metro station for each property ---\n",
        "# Convert our property DataFrame to a GeoDataFrame\n",
        "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
        "gdf_properties = gpd.GeoDataFrame(df, geometry=geometry, crs=metro_stations.crs)\n",
        "\n",
        "# --- THE FIX: Calculate the centroid for all station geometries ---\n",
        "# This ensures that every station, whether it's a Point or a Polygon,\n",
        "# is represented by a single point (its center).\n",
        "station_centroids = metro_stations.geometry.centroid\n",
        "\n",
        "# Prepare the coordinates for efficient searching, using the centroids\n",
        "station_coords = np.array(list(zip(station_centroids.y, station_centroids.x)))\n",
        "property_coords = np.array(list(zip(gdf_properties.geometry.y, gdf_properties.geometry.x)))\n",
        "\n",
        "# Use cKDTree for a very fast nearest-neighbor search\n",
        "kdtree = cKDTree(station_coords)\n",
        "dist, idx = kdtree.query(property_coords, k=1)\n",
        "\n",
        "# The result 'dist' is in degrees, so we convert it to kilometers (approx)\n",
        "df['dist_to_metro_km'] = dist * 111.1\n",
        "\n",
        "print(\"\\n'dist_to_metro_km' column created.\")\n",
        "print(df[['location', 'dist_to_metro_km']].head())\n",
        "\n",
        "# Save our final, fully-enriched dataset!\n",
        "df.to_csv('enriched_mumbai_data.csv', index=False)\n",
        "\n",
        "print(\"\\n--- Geospatial Feature Engineering Complete! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_7Uk1K_Rt1z",
        "outputId": "fbb016b6-31ad-4d41-b575-33da1353bbe6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: osmnx in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from osmnx) (3.5)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.32.3)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.11/dist-packages (from osmnx) (2.1.1)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->osmnx) (2025.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27->osmnx) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->osmnx) (1.17.0)\n",
            "Downloading metro station data from OpenStreetMap...\n",
            "Found 88 metro stations.\n",
            "\n",
            "Types of geometries found for stations:\n",
            "Point      72\n",
            "Polygon    16\n",
            "Name: count, dtype: int64\n",
            "\n",
            "'dist_to_metro_km' column created.\n",
            "  location  dist_to_metro_km\n",
            "0   Airoli          5.029939\n",
            "1   Airoli          5.029939\n",
            "2   Airoli          5.029939\n",
            "3   Airoli          5.029939\n",
            "4   Airoli          5.029939\n",
            "\n",
            "--- Geospatial Feature Engineering Complete! ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-3082006999.py:31: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  station_centroids = metro_stations.geometry.centroid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Import Libraries and Load Data ---\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import joblib # For saving the model\n",
        "\n",
        "# Load our final, feature-rich dataset\n",
        "df = pd.read_csv('enriched_mumbai_data.csv')\n",
        "\n",
        "print(\"Dataset loaded. Shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# --- Step 2: Prepare the Data for Machine Learning ---\n",
        "# Machine learning models need all input to be numeric.\n",
        "# Our 'location' column is text, so we need to convert it.\n",
        "# The best way to do this is with One-Hot Encoding.\n",
        "\n",
        "# 'pd.get_dummies' creates a new column for each location with a 1 or 0.\n",
        "df_encoded = pd.get_dummies(df, columns=['location'], drop_first=True)\n",
        "\n",
        "print(\"\\nShape after one-hot encoding:\", df_encoded.shape)\n",
        "print(\"New columns created for locations.\")\n",
        "\n",
        "# Define our features (X) and our target (y)\n",
        "X = df_encoded.drop('price', axis='columns')\n",
        "y = df_encoded['price']\n",
        "\n",
        "\n",
        "# --- Step 3: Split Data into Training and Testing Sets ---\n",
        "# We train the model on the training set and then test its performance\n",
        "# on the unseen testing set to get an honest evaluation.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining set has {X_train.shape[0]} samples.\")\n",
        "print(f\"Testing set has {X_test.shape[0]} samples.\")\n",
        "\n",
        "\n",
        "# --- Step 4: Initialize and Train the XGBoost Model ---\n",
        "print(\"\\nTraining the XGBoost model...\")\n",
        "\n",
        "# Initialize the XGBoost Regressor model\n",
        "# A regressor is used because we are predicting a continuous value (price)\n",
        "model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "\n",
        "# Train the model on our training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete!\")\n",
        "\n",
        "\n",
        "# --- Step 5: Evaluate the Model's Performance ---\n",
        "print(\"\\nEvaluating model performance on the test set...\")\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(f\"R-squared (R²): {r2:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f} Lakhs\")\n",
        "\n",
        "# Let's interpret the results in plain English\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"The R-squared score means our model can explain approximately {r2:.0%} of the variance in house prices.\")\n",
        "print(f\"The Mean Absolute Error means that, on average, our model's price prediction is off by about {mae:.2f} Lakhs.\")\n",
        "\n",
        "\n",
        "# --- Step 6: Save the Trained Model to a File ---\n",
        "# This is a CRITICAL step. We save the 'trained brain' so we can\n",
        "# load it directly into our dashboard later without retraining.\n",
        "joblib.dump(model, 'mumbai_price_model.joblib')\n",
        "\n",
        "print(\"\\nModel has been saved to 'mumbai_price_model.joblib'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kKWCySOZnIQ",
        "outputId": "6a34976e-3cd9-4017-b694-802d879a7c07"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Shape: (2922, 43)\n",
            "      price  area location  no_of_bedrooms  resale  maintenancestaff  \\\n",
            "0   6200000   400   Airoli               1       1                 1   \n",
            "1   9500000  1000   Airoli               2       1                 1   \n",
            "2  14900000  1245   Airoli               2       0                 0   \n",
            "3  14000000  1183   Airoli               2       0                 0   \n",
            "4   6400000   495   Airoli               1       1                 9   \n",
            "\n",
            "   gymnasium  swimmingpool  landscapedgardens  joggingtrack  ...  microwave  \\\n",
            "0          0             0                  0             0  ...          0   \n",
            "1          0             1                  0             0  ...          0   \n",
            "2          0             0                  0             0  ...          0   \n",
            "3          1             1                  1             1  ...          0   \n",
            "4          9             9                  9             9  ...          9   \n",
            "\n",
            "   golfcourse  tv  diningtable  sofa  wardrobe  refrigerator   latitude  \\\n",
            "0           0   0            0     0         0             0  19.158515   \n",
            "1           0   0            0     0         0             0  19.158515   \n",
            "2           0   0            0     0         0             0  19.158515   \n",
            "3           0   0            0     0         0             0  19.158515   \n",
            "4           9   9            9     9         9             9  19.158515   \n",
            "\n",
            "   longitude  dist_to_metro_km  \n",
            "0  72.999402          5.029939  \n",
            "1  72.999402          5.029939  \n",
            "2  72.999402          5.029939  \n",
            "3  72.999402          5.029939  \n",
            "4  72.999402          5.029939  \n",
            "\n",
            "[5 rows x 43 columns]\n",
            "\n",
            "Shape after one-hot encoding: (2922, 109)\n",
            "New columns created for locations.\n",
            "\n",
            "Training set has 2337 samples.\n",
            "Testing set has 585 samples.\n",
            "\n",
            "Training the XGBoost model...\n",
            "Model training complete!\n",
            "\n",
            "Evaluating model performance on the test set...\n",
            "R-squared (R²): 0.75\n",
            "Mean Absolute Error (MAE): 2974453.50 Lakhs\n",
            "\n",
            "Interpretation:\n",
            "The R-squared score means our model can explain approximately 75% of the variance in house prices.\n",
            "The Mean Absolute Error means that, on average, our model's price prediction is off by about 2974453.50 Lakhs.\n",
            "\n",
            "Model has been saved to 'mumbai_price_model.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Import Libraries and Load Data ---\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import numpy as np # For log transform\n",
        "import joblib\n",
        "\n",
        "# Load our final, feature-rich dataset\n",
        "df = pd.read_csv('enriched_mumbai_data.csv')\n",
        "print(\"Dataset loaded. Shape:\", df.shape)\n",
        "\n",
        "# --- Step 2: Prepare Data for Machine Learning (with a key improvement) ---\n",
        "# One-Hot Encode the 'location' column\n",
        "df_encoded = pd.get_dummies(df, columns=['location'], drop_first=True)\n",
        "print(\"\\nShape after one-hot encoding:\", df_encoded.shape)\n",
        "\n",
        "# IMPROVEMENT: Use a Log Transform on the Price\n",
        "# Price data is often \"right-skewed\" (many cheaper properties, few very expensive ones).\n",
        "# A log transform makes the distribution more normal and helps the model perform better.\n",
        "df_encoded['price_log'] = np.log(df_encoded['price'])\n",
        "\n",
        "# Define features (X) and our NEW log-transformed target (y)\n",
        "X = df_encoded.drop(['price', 'price_log'], axis='columns') # Drop both price columns from features\n",
        "y = df_encoded['price_log'] # Our target is now the log of the price\n",
        "\n",
        "\n",
        "# --- Step 3: Split Data into Training and Testing Sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"\\nTraining set has {X_train.shape[0]} samples.\")\n",
        "\n",
        "\n",
        "# --- Step 4: Initialize and Train the Improved XGBoost Model ---\n",
        "print(\"\\nTraining the XGBoost model...\")\n",
        "# We can use slightly more powerful parameters\n",
        "model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, subsample=0.8, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete!\")\n",
        "\n",
        "\n",
        "# --- Step 5: Evaluate the Model's Performance (with corrected logic) ---\n",
        "print(\"\\nEvaluating model performance on the test set...\")\n",
        "# The model predicts the LOG of the price\n",
        "log_predictions = model.predict(X_test)\n",
        "\n",
        "# We must convert the predictions back to actual prices using the inverse of log (exponent)\n",
        "predictions = np.exp(log_predictions)\n",
        "# The original test prices also need to be converted back from log form\n",
        "y_test_actual = np.exp(y_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "r2 = r2_score(y_test_actual, predictions)\n",
        "mae = mean_absolute_error(y_test_actual, predictions)\n",
        "mae_lakhs = mae / 100000 # Convert MAE from Rupees to Lakhs for easier interpretation\n",
        "\n",
        "print(f\"R-squared (R²): {r2:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_lakhs:.2f} Lakhs\") # THE FIX IS HERE\n",
        "\n",
        "# Let's interpret the results in plain English\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"The R-squared score means our model can explain approximately {r2:.0%} of the variance in house prices.\")\n",
        "print(f\"The Mean Absolute Error means that, on average, our model's price prediction is off by about {mae_lakhs:.2f} Lakhs.\")\n",
        "\n",
        "\n",
        "# --- Step 6: Save the Trained Model to a File ---\n",
        "joblib.dump(model, 'mumbai_price_model.joblib')\n",
        "print(\"\\nModel has been saved to 'mumbai_price_model.joblib'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30h1QX14ZnLP",
        "outputId": "4d6f4754-fd8e-4f38-fb7b-a3389ece6a86"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Shape: (2922, 43)\n",
            "\n",
            "Shape after one-hot encoding: (2922, 109)\n",
            "\n",
            "Training set has 2337 samples.\n",
            "\n",
            "Training the XGBoost model...\n",
            "Model training complete!\n",
            "\n",
            "Evaluating model performance on the test set...\n",
            "R-squared (R²): 0.76\n",
            "Mean Absolute Error (MAE): 28.32 Lakhs\n",
            "\n",
            "Interpretation:\n",
            "The R-squared score means our model can explain approximately 76% of the variance in house prices.\n",
            "The Mean Absolute Error means that, on average, our model's price prediction is off by about 28.32 Lakhs.\n",
            "\n",
            "Model has been saved to 'mumbai_price_model.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Import Libraries and Load Data ---\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load our final, feature-rich dataset\n",
        "df = pd.read_csv('enriched_mumbai_data.csv')\n",
        "print(\"Dataset loaded. Shape:\", df.shape)\n",
        "\n",
        "# --- Step 2: Prepare Data for Machine Learning ---\n",
        "df_encoded = pd.get_dummies(df, columns=['location'], drop_first=True)\n",
        "df_encoded['price_log'] = np.log(df_encoded['price'])\n",
        "X = df_encoded.drop(['price', 'price_log'], axis='columns')\n",
        "y = df_encoded['price_log']\n",
        "\n",
        "# --- Step 3: Split Data into Training and Testing Sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"\\nTraining set has {X_train.shape[0]} samples.\")\n",
        "\n",
        "# --- Step 4: Hyperparameter Tuning with GridSearchCV ---\n",
        "print(\"\\nStarting hyperparameter tuning with GridSearchCV... This may take several minutes.\")\n",
        "\n",
        "# Define the grid of parameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [300, 500, 700],\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.7, 0.8],\n",
        "    'colsample_bytree': [0.7, 0.8]\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# cv=3 means 3-fold cross-validation.\n",
        "# scoring='r2' tells it to optimize for the best R-squared score.\n",
        "# verbose=2 will print progress updates.\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='r2', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete!\")\n",
        "\n",
        "# Get the best model found by the search\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"\\nBest Hyperparameters Found: {grid_search.best_params_}\")\n",
        "\n",
        "\n",
        "# --- Step 5: Evaluate the BEST Model's Performance ---\n",
        "print(\"\\nEvaluating the optimized model on the test set...\")\n",
        "log_predictions = best_model.predict(X_test)\n",
        "predictions = np.exp(log_predictions)\n",
        "y_test_actual = np.exp(y_test)\n",
        "\n",
        "r2 = r2_score(y_test_actual, predictions)\n",
        "mae = mean_absolute_error(y_test_actual, predictions)\n",
        "mae_lakhs = mae / 100000\n",
        "\n",
        "print(f\"Optimized R-squared (R²): {r2:.2f}\")\n",
        "print(f\"Optimized Mean Absolute Error (MAE): {mae_lakhs:.2f} Lakhs\")\n",
        "\n",
        "# --- Step 6: Save the OPTIMIZED Model to a File ---\n",
        "joblib.dump(best_model, 'mumbai_price_model_optimized.joblib')\n",
        "print(\"\\nOptimized model has been saved to 'mumbai_price_model_optimized.joblib'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHkBSDp5ZnOU",
        "outputId": "c12be32a-a059-43e7-e4f4-09581f014600"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Shape: (2922, 43)\n",
            "\n",
            "Training set has 2337 samples.\n",
            "\n",
            "Starting hyperparameter tuning with GridSearchCV... This may take several minutes.\n",
            "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
            "\n",
            "Hyperparameter tuning complete!\n",
            "\n",
            "Best Hyperparameters Found: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.8}\n",
            "\n",
            "Evaluating the optimized model on the test set...\n",
            "Optimized R-squared (R²): 0.76\n",
            "Optimized Mean Absolute Error (MAE): 28.56 Lakhs\n",
            "\n",
            "Optimized model has been saved to 'mumbai_price_model_optimized.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the trained model itself\n",
        "model = joblib.load('mumbai_price_model_optimized.joblib')\n",
        "\n",
        "# Access the model's internal list of feature names that it was trained on\n",
        "# This is the GUARANTEED correct list.\n",
        "correct_training_columns = model.feature_names_in_\n",
        "\n",
        "# Save this foolproof list to our blueprint file\n",
        "joblib.dump(correct_training_columns, 'training_columns.joblib')\n",
        "\n",
        "print(\"A new, 100% correct 'training_columns.joblib' has been created.\")\n",
        "print(\"Please download this file and replace the old one in your project folder.\")\n",
        "print(f\"The model expects exactly {len(correct_training_columns)} features.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybbT-PvnjzHr",
        "outputId": "798f2e83-7b0e-49fd-f819-7bed047c46fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A new, 100% correct 'training_columns.joblib' has been created.\n",
            "Please download this file and replace the old one in your project folder.\n",
            "The model expects exactly 108 features.\n"
          ]
        }
      ]
    }
  ]
}